{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec0f287",
   "metadata": {},
   "source": [
    "# Import Necessary Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3de9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PREPROCESSING FUNCTIONS FOR USE IN MODEL DEVELOPMENT, EVALUATION, AND PRODUCTION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL as pil\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tempfile\n",
    "import pickle\n",
    "import time\n",
    "import gc\n",
    "import skimage.filters\n",
    "import cv2\n",
    "import watermark\n",
    "import joblib\n",
    "import math\n",
    "from skimage.measure import block_reduce\n",
    "from image_preprocessing import standardize_image_dataset,resize_dataset,binarize_dataset,crop_dataset,process_dataset_blur,do_pooling_dataset\n",
    "from pipeline import model_pipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV,KFold\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefeb3ac",
   "metadata": {},
   "source": [
    "# Read in Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1614a960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pickle.load(open('Amit/Labeled Data/train_data.pkl','rb'))\n",
    "train_x,train_y = all_data.iloc[:,:-1],all_data.iloc[:,-1]\n",
    "del all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cdca25",
   "metadata": {},
   "source": [
    "# Automated Model Development Pipeline\n",
    "\n",
    "Functional wrapper around model_pipeline class allowing for the automated identification of an optimal model and hyperparameter settings in conjunction with a greedy approach for identifying optimal preprocessing steps. \n",
    "\n",
    "The greedy approach is achieved by first identifying an optimal image size, or an image size that yields the best performing model without being too large to the point where it dramatically slows down training time and potentially reduces optimal model performance. Next, a specific preprocessing methodology is incorporated across a variety of preprocessing settings. For each of these settings, our image vectors / features are preprocessed according to these settings and an optimal model is identified. At the completion of evaluating a specific preprocessing methodology, if the optimal model identified is better than the previously identified optimal model trained on resized image data, the optimal model parameters are replaced. In addition, the features this model was trained on according to associated preprocessing settings are permanently applied to the features (Greedy). This process continues with other preprocessing methodologies as defined by the user, where at each step the pipeline identifies whether any additional preprocessing steps yield an improved model given previously incorporated preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c639d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_optimal_model_dev(X,y,model,param_grid,preprocessing_eval_order = ['bin/crop','blur','pool'],resize=True):\n",
    "    \n",
    "    #Store global values to capture optimal model/parameters, key performance metrics, and optimal preprocessing steps\n",
    "    best_feats = X.copy()\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_probs = None\n",
    "    best_preds = None\n",
    "    best_thresh = None\n",
    "    best_score = 0\n",
    "    best_preprocess = '(Initial Standardization/Resizing to ' + str((int(np.sqrt(X.shape[1])),int(np.sqrt(X.shape[1])))) + ')'\n",
    "    # Initial Evaluation - Identify Optimal Size of Images, measured by performance of optimal model yielded by training on \n",
    "    #images of various\n",
    "    \n",
    "    if resize == True:\n",
    "\n",
    "        for img_size in [(2**num,2**num) for num in range(4,int(math.log2(np.sqrt(X.shape[1]))) + 1)]:\n",
    "\n",
    "            resize_results = model_pipeline().evaluate(resize_dataset(X,(int(np.sqrt(X.shape[1])),int(np.sqrt(X.shape[1]))),img_size),\n",
    "                                                       y,preprocessing=[],model=model,param_grid=param_grid,\n",
    "                                                       optimizing_metric='f1',n_splits=5,return_transformed_features=True,\n",
    "                                                       return_grid=True,return_score=True,return_best_estimator=True,\n",
    "                                                       return_best_params=True,return_oos_pred=True,return_oos_prob=True,\n",
    "                                                       return_threshold_analysis=True)\n",
    "            score = resize_results['threshold_analysis']['best_score'] #get f1 score of best performing model trained on images\n",
    "            #of specified size\n",
    "\n",
    "            if score - best_score > 0.001:\n",
    "\n",
    "            #Extract key results and the model for best performing model if model is at minimum > 0.01 in F1 score performance\n",
    "            # then previously identified optimal model\n",
    "                best_feats = resize_results['features']\n",
    "                best_model = resize_results['best_estimator']\n",
    "                best_params = resize_results['best_params']\n",
    "                best_probs = resize_results['oos_probs']\n",
    "                best_preds = resize_results['threshold_analysis']['best_preds']\n",
    "                best_thresh = resize_results['threshold_analysis']['best_thresh']\n",
    "                best_score = score\n",
    "                best_preprocess = '(Initial Standardization/Resizing to ' + str(img_size) + ')'\n",
    "                print('Better Model Identified by Resizing Images to ' + str(img_size) + ': ' + str(score))\n",
    "            else: \n",
    "                #break early if increasing image size does not yield a significantly better performing optimal model\n",
    "                break\n",
    "    \n",
    "    #detect size of images for binarization/cropping and blur preprocessing steps\n",
    "    image_size = int(np.sqrt(best_feats.shape[1]))\n",
    "    \n",
    "    binarization_crop_settings = [[('binarize',[True,0.3]),('crop',[(image_size,image_size),(image_size,image_size)])],\n",
    "                                         [('binarize',[False,0.05]),('crop',[(image_size,image_size),(image_size,image_size)])],\n",
    "                                         [('binarize',[False,0.1]),('crop',[(image_size,image_size),(image_size,image_size)])],\n",
    "                                         [('binarize',[False,0.15]),('crop',[(image_size,image_size),(image_size,image_size)])],\n",
    "                                         [('binarize',[False,0.2]),('crop',[(image_size,image_size),(image_size,image_size)])],\n",
    "                                         [('binarize',[False,0.3]),('crop',[(image_size,image_size),(image_size,image_size)])]]\n",
    "    \n",
    "    blur_settings = [[('blur',['g',(image_size,image_size),(3,3),0,0])],\n",
    "                     [('blur',['g',(image_size,image_size),(3,3),1,0])],\n",
    "                     [('blur',['g',(image_size,image_size),(3,3),0,1])],\n",
    "                     [('blur',['g',(image_size,image_size),(3,3),1,1])],\n",
    "                     [('blur',['g',(image_size,image_size),(3,3),2,2])],\n",
    "                     [('blur',['g',(image_size,image_size),(5,5),0,0])],\n",
    "                     [('blur',['g',(image_size,image_size),(5,5),1,0])],\n",
    "                     [('blur',['g',(image_size,image_size),(5,5),0,1])],\n",
    "                     [('blur',['g',(image_size,image_size),(5,5),1,1])],\n",
    "                     [('blur',['g',(image_size,image_size),(5,5),2,2])],\n",
    "                     [('blur',['b',(image_size,image_size),(3,3),0,0])],\n",
    "                     [('blur',['b',(image_size,image_size),(3,3),1,0])],\n",
    "                     [('blur',['b',(image_size,image_size),(3,3),0,1])],\n",
    "                     [('blur',['b',(image_size,image_size),(3,3),1,1])],\n",
    "                     [('blur',['b',(image_size,image_size),(3,3),2,2])],\n",
    "                     [('blur',['b',(image_size,image_size),(5,5),0,0])],\n",
    "                     [('blur',['b',(image_size,image_size),(5,5),1,0])],\n",
    "                     [('blur',['b',(image_size,image_size),(5,5),0,1])],\n",
    "                     [('blur',['b',(image_size,image_size),(5,5),1,1])],\n",
    "                     [('blur',['b',(image_size,image_size),(5,5),2,2])]]\n",
    "    \n",
    "    \n",
    "    #detect possible pooling settings dependent on image_size, controls possible pool sizes\n",
    "    pool_ranges = int(math.log2(image_size))\n",
    "    pool_settings = []\n",
    "    for num in range(1,pool_ranges):\n",
    "        pool_settings.append([('pool',[(2**num,2**num),np.max])])\n",
    "        pool_settings.append([('pool',[(2**num,2**num),np.mean])])\n",
    "    \n",
    "    for step in preprocessing_eval_order:\n",
    "        #Identify optimal model considering different image preprocessing settings to also identify\n",
    "        #optimal preprocessing settings\n",
    "        \n",
    "        #set settings we will evaluate depending on the user defined preprocessing evaluation order\n",
    "        if step == 'bin/crop':\n",
    "            settings = binarization_crop_settings\n",
    "        elif step == 'blur':\n",
    "            settings = blur_settings\n",
    "        elif step == 'pool':\n",
    "            settings = pool_settings\n",
    "        \n",
    "        best_setting = ''\n",
    "        best_setting_feats = None\n",
    "\n",
    "        #For each preprocessing setting, identify an optimal performing model trained on\n",
    "        #transformed features according to specified preprocessing. Compare each model to currently identified\n",
    "        #optimal model and replace if better model is found\n",
    "        for setting in settings:\n",
    "            if step == 'bin/crop' and int(np.sqrt(best_feats.shape[1])) != image_size: #if pooling was evaluated first and\n",
    "                #yielded a model better than base case, resulting data would have been resized so dimension settings for\n",
    "                #binarization, cropping will need to be adjusted\n",
    "                new_image_size = int(np.sqrt(best_feats.shape[1]))\n",
    "                setting[1][1][0] = (new_image_size,new_image_size)\n",
    "                setting[1][1][1] = (new_image_size,new_image_size)\n",
    "            elif step == 'blur' and int(np.sqrt(best_feats.shape[1])) != image_size: #same case as above but for blurring\n",
    "                new_image_size = int(np.sqrt(best_feats.shape[1]))\n",
    "                setting[0][1][1] = (new_image_size,new_image_size)\n",
    "            setting_case = model_pipeline().evaluate(best_feats,y,preprocessing=setting,model=model,param_grid=param_grid,\n",
    "                                                      optimizing_metric='f1',n_splits=5,return_transformed_features=True,\n",
    "                                                      return_grid=True,return_score=True,return_best_estimator=True,\n",
    "                                                      return_best_params=True,return_oos_pred=True,return_oos_prob=True,\n",
    "                                                      return_threshold_analysis=True)\n",
    "            score = setting_case['threshold_analysis']['best_score']#get F1 score of optimal model trained using preprocessed features\n",
    "            if score > best_score: #if score is better than current best score, update key results and model for optimal performing model\n",
    "                best_model = setting_case['best_estimator']\n",
    "                best_params = setting_case['best_params']\n",
    "                best_probs = setting_case['oos_probs']\n",
    "                best_preds = setting_case['threshold_analysis']['best_preds']\n",
    "                best_thresh = setting_case['threshold_analysis']['best_thresh']\n",
    "                best_score = score\n",
    "                best_setting_feats = setting_case['features']\n",
    "                if step == 'bin/crop':\n",
    "                    best_setting = '(Binarization, Automate Threshold = ' + str(setting[0][1][0]) + ', Threshold = ' + str(setting[0][1][1]) + ') (Crop, ' + str(setting[1][1][0]) + ', ' + str(setting[1][1][0]) + ')'\n",
    "                    print('Better Model Identified W/ Binarization/Cropping, Score = ' + str(score))\n",
    "                elif step == 'blur':\n",
    "                    best_setting = '(Blurring, Type = ' + str(setting[0][1][0]) + ', Dimension = ' + str(setting[0][1][1]) + ', Kernel = ' + str(setting[0][1][2]) + ', sigma_x = ' + str(setting[0][1][3]) + ', sigma_y = ' + str(setting[0][1][4]) + ')'\n",
    "                    print('Better Model Identified W/ Blurring, Score = ' + str(score))\n",
    "                elif step == 'pool':\n",
    "                    best_setting = '(Pool, pool_size = ' + str(setting[0][1][0]) + ', pooling_function = ' + str(setting[0][1][1]) + ')'\n",
    "                    print('Better Model Identified W/ Pooling, Score = ' + str(score))\n",
    "                    \n",
    "\n",
    "        #Update features and preprocessing string if incorporating specific preprocessing as part of image preprocessing pipeline yielded \n",
    "        #a better performing model. This ensures these steps do not need to be repeated when evaluating additional \n",
    "        #preprocessing steps\n",
    "        if best_setting != '':\n",
    "            best_feats = best_setting_feats\n",
    "            best_preprocess = best_preprocess + best_setting\n",
    "    \n",
    "    \n",
    "    #store and return optimal model, threshold, out of sample predictions, features the model was trained on, \n",
    "    #and optimal preprocessing steps identified via a greedy sequential decision process\n",
    "    return_dict = {}\n",
    "    return_dict['features'] = best_feats\n",
    "    return_dict['best_model'] = best_model\n",
    "    return_dict['best_params'] = best_params\n",
    "    return_dict['oos_probs'] = best_probs\n",
    "    return_dict['oos_preds'] = best_preds\n",
    "    return_dict['best_thresh'] = best_thresh\n",
    "    return_dict['best_score'] = best_score\n",
    "    return_dict['best_preprocess'] = best_preprocess\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fda101",
   "metadata": {},
   "source": [
    "# Identify Optimal Random Forest Alongside Optimized Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9fb868",
   "metadata": {},
   "source": [
    "[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]  \n",
    "1 = bin/crop  \n",
    "2 = blur  \n",
    "3 = pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6076f",
   "metadata": {},
   "source": [
    "### Test 1: Identify optimal Random Forest while sequentially identifying optimal settings for bin/crop, blur, and pooling [3,2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b2b0d",
   "metadata": {},
   "source": [
    "test_1 = automate_optimal_model_dev(X = train_x,y = train_y,\n",
    "                                    model = GradientBoostingClassifier(n_estimators=500,random_state=50,max_features=\"sqrt\"),\n",
    "                                    param_grid = {'max_depth':[2,3,4]},\n",
    "                                    preprocessing_eval_order = ['pool','blur','bin/crop'])\n",
    "test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb969e5",
   "metadata": {},
   "source": [
    "pickle.dump(test_1,open('Partition Based Model Results/gbct1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fb4bb",
   "metadata": {},
   "source": [
    "### Test 2: Identify optimal Random Forest while sequentially identifying optimal settings for bin/crop, blur, and pooling [3,1,2], Test 1 yielded that resize value of 32x32 is optimal and pooling can be optimized after with a 2x2 np.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f6268",
   "metadata": {},
   "source": [
    "test_2 = automate_optimal_model_dev(X = resize_dataset(train_x,(256,256),(32,32)),y = train_y,\n",
    "                                    model = GradientBoostingClassifier(n_estimators=500,random_state=50,max_features=\"sqrt\"),\n",
    "                                    param_grid = {'max_depth':[2,3,4]},\n",
    "                                    preprocessing_eval_order = ['bin/crop','blur'],resize=False)\n",
    "test_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87aa96f",
   "metadata": {},
   "source": [
    "pickle.dump(test_2,open('Partition Based Model Results/gbct2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4764979",
   "metadata": {},
   "source": [
    "### Test 3: Identify optimal Random Forest while sequentially identifying optimal settings for bin/crop, blur, and pooling [2,1,3], test 1 yielded resize of 32x32 as optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cab05",
   "metadata": {},
   "source": [
    "test_3 = automate_optimal_model_dev(X = resize_dataset(train_x,(256,256),(32,32)),y = train_y,\n",
    "                                    model = GradientBoostingClassifier(n_estimators=500,random_state=50,max_features=\"sqrt\"),\n",
    "                                    param_grid = {'max_depth':[2,3,4]},\n",
    "                                    preprocessing_eval_order = ['blur','bin/crop','pool'],resize=False)\n",
    "test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c23181b",
   "metadata": {},
   "source": [
    "### Test 4: Identify optimal Random Forest while sequentially identifying optimal settings for bin/crop, blur, and pooling [1,2,3], test 1 yielded resize of 32x32 as optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6559d",
   "metadata": {},
   "source": [
    "test_4 = automate_optimal_model_dev(X = resize_dataset(train_x,(256,256),(32,32)),y = train_y,\n",
    "                                    model = GradientBoostingClassifier(n_estimators=500,random_state=50,max_features=\"sqrt\"),\n",
    "                                    param_grid = {'max_depth':[2,3,4]},\n",
    "                                    preprocessing_eval_order = ['bin/crop','blur','pool'],resize=False)\n",
    "test_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b69438e",
   "metadata": {},
   "source": [
    "### Test 5: Identify optimal Random Forest while sequentially identifying optimal settings for bin/crop, blur, and pooling [1,3,2], test 1 yielded resize of 32x32 as optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba972f",
   "metadata": {},
   "source": [
    "test_5 = automate_optimal_model_dev(X = resize_dataset(train_x,(256,256),(32,32)),y = train_y,\n",
    "                                    model = GradientBoostingClassifier(n_estimators=500,random_state=50,max_features=\"sqrt\"),\n",
    "                                    param_grid = {'max_depth':[2,3,4]},\n",
    "                                    preprocessing_eval_order = ['bin/crop','pool','blur'],resize=False)\n",
    "test_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba07851",
   "metadata": {},
   "source": [
    "### Test 6: Identify optimal Random Forest while sequentially identifying optimal settings for bin/crop, blur, and pooling [2,3,1], test 1 yielded resize of 32x32 as optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d8e9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better Model Identified by Resizing Images to (16, 16): 0.9703247480403135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'features':       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       " 54         0       0       1       1       0       8      24      28      26   \n",
       " 2602       2       2       3       3       2       4      23      36      30   \n",
       " 3433       0       0       2       2       1       1       0       0       0   \n",
       " 235        0      12      38      30      43      61      54      53      54   \n",
       " 1806       0       2       2       3       3       3       3       3       3   \n",
       " ...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       " 3330       0       0       0       0       0       0       0       0       0   \n",
       " 70        23       1       4       5       4       3       7      11      12   \n",
       " 132        0       0       0       0       0       6      25      47      60   \n",
       " 2014       4       4       4       4       3       3       4       4       4   \n",
       " 1931       1       1       1       0       0      56     117     144     149   \n",
       " \n",
       "       pixel9  ...  pixel246  pixel247  pixel248  pixel249  pixel250  pixel251  \\\n",
       " 54         9  ...        50        73        70        35         2         0   \n",
       " 2602      27  ...        60        86        85        57        41         8   \n",
       " 3433       0  ...        93       115       117        93        48         1   \n",
       " 235       68  ...        54        89        89        79        59        36   \n",
       " 1806       3  ...       112       105       103       106        91        78   \n",
       " ...      ...  ...       ...       ...       ...       ...       ...       ...   \n",
       " 3330       0  ...         0         0         0         0         0         0   \n",
       " 70         8  ...        15        22        24        17        12        12   \n",
       " 132       45  ...        43        55        51        41        16         0   \n",
       " 2014       4  ...       105       119        85       114       118       129   \n",
       " 1931     162  ...       126       125       118       109       104        89   \n",
       " \n",
       "       pixel252  pixel253  pixel254  pixel255  \n",
       " 54           1         1         0         0  \n",
       " 2602         1         3         3         3  \n",
       " 3433         0         2         0         0  \n",
       " 235         31        39        15         0  \n",
       " 1806       139        17         1         0  \n",
       " ...        ...       ...       ...       ...  \n",
       " 3330         0         0         0         0  \n",
       " 70          12        11        11        10  \n",
       " 132          0         0         0         0  \n",
       " 2014        98        45         1         5  \n",
       " 1931        24         0         1         1  \n",
       " \n",
       " [3220 rows x 256 columns],\n",
       " 'best_model': GradientBoostingClassifier(max_depth=4, max_features='sqrt', n_estimators=500,\n",
       "                            random_state=50),\n",
       " 'best_params': {'max_depth': 4},\n",
       " 'oos_probs': 54      0.995538\n",
       " 2602    0.072117\n",
       " 3433    0.008864\n",
       " 235     0.997426\n",
       " 1806    0.989194\n",
       "           ...   \n",
       " 3330    0.001854\n",
       " 70      0.682469\n",
       " 132     0.941693\n",
       " 2014    0.999247\n",
       " 1931    0.993247\n",
       " Name: label, Length: 3220, dtype: float64,\n",
       " 'oos_preds': array([1, 0, 0, ..., 1, 1, 1]),\n",
       " 'best_thresh': 0.51,\n",
       " 'best_score': 0.9703247480403135,\n",
       " 'best_preprocess': '(Initial Standardization/Resizing to (16, 16))'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_6 = automate_optimal_model_dev(X = resize_dataset(train_x,(256,256),(32,32)),y = train_y,\n",
    "                                    model = GradientBoostingClassifier(n_estimators=500,random_state=50,max_features=\"sqrt\"),\n",
    "                                    param_grid = {'max_depth':[2,3,4]},\n",
    "                                    preprocessing_eval_order = ['blur','pool','bin/crop'],resize=True)\n",
    "test_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff3fc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_6,open('Partition Based Model Results/gbct1.pkl','wb'))\n",
    "#pickle.dump(test_7,open('Partition Based Model Results/gbct2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e649b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
